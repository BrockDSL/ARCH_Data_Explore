{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMM 4P35 - Web Archives Tutorial\n",
    "\n",
    "\n",
    "## Part 1 - Analyzing changes to Canada.ca pages\n",
    "\n",
    "This notebook uses a subset of the the data from the [COVID in Niagara Archive](https://archive-it.org/collections/13781). We'll use Google Collab to explore how some pages from the [canada.ca](https://canada.ca) domain have changed during the course of the pandemic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the the pieces\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "\n",
    "import difflib\n",
    "from IPython import display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.\n",
    "\n",
    "We'll load up the CSV file of data that represents our crawls of the canada.ca pages and randomly display one row of this spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "web_page_text = pd.read_csv(\"https://raw.githubusercontent.com/BrockDSL/ARCH_Data_Explore/main/snap_shot_canada_ca.csv\")\n",
    "\n",
    "web_page_text['crawl_date']= pd.to_datetime(web_page_text['crawl_date'],format='%Y%m%d')\n",
    "#add an extra column with how the length of each crawl. Useful for later calculations\n",
    "for index, row in web_page_text.iterrows():\n",
    "    web_page_text.at[index, \"length\"] = len(web_page_text.at[index,\"content\"])\n",
    "web_page_text.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of web pages captures in this archive subset: \" + str(len(web_page_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 \n",
    "\n",
    "Let's look at how many times the top 25 URLs in this archive have been crawled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_page_text.groupby([\"url\"]).count().sort_values(by=\"crawl_date\",ascending=False)[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Let's look at a specific URL... We set it in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.canada.ca/en/public-health/services/diseases/2019-novel-coronavirus-infection/prevention-risks/covid-19-improving-indoor-ventilation.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... with that set, let's plot out the change in content length of that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data = web_page_text[web_page_text['url'] == URL].sort_values(by=\"crawl_date\")\n",
    "\n",
    "plt.plot(url_data['crawl_date'],url_data['length'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Word count variation by crawl for \\n\" + URL)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "\n",
    "Curious. We see a huge step in page length.\n",
    "\n",
    "Let's open up both version of this page on the Internet Archive and see if we can spot the difference in the pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_page = url_data[url_data['length'] == url_data['length'].max()]\n",
    "max_page_date = str(max_page['crawl_date'].values[0]).split('T')[0].replace('-','')\n",
    "\n",
    "\n",
    "print(\"\\n\\nLongest version of this page on the Internet Archive was captured \"\\\n",
    "      + max_page_date + \"\\n\" \\\n",
    "      + \"Open this version on Internet Archive \\n\"\n",
    "      + \"https://web.archive.org/web/\" \\\n",
    "      + max_page_date + \"/\" + URL)\n",
    "\n",
    "\n",
    "\n",
    "min_page = url_data[url_data['length'] == url_data['length'].min()]\n",
    "min_page_date = str(min_page['crawl_date'].values[0]).split('T')[0].replace('-','')\n",
    "\n",
    "\n",
    "print(\"\\n\\nShortest version of this page on the Internet Archive was captured \"\\\n",
    "      + min_page_date + \"\\n\" \\\n",
    "      + \"Open this version on Internet Archive \\n\"\n",
    "      + \"https://web.archive.org/web/\" \\\n",
    "      + min_page_date + \"/\" + URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "We can use an algorithm to determine the sentiment expressed in the page. This is measured by two scores:\n",
    "- Polarity - closer to 0 means more 'negative' sentiment\n",
    "- Subjectivity - closer to 0 means a more objective statement\n",
    "\n",
    "Let's compare these two dimensions for the *Longest Length* page and the *Shortest Length* page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_page_sa = TextBlob(str(max_page['content'])).sentiment\n",
    "min_page_sa = TextBlob(str(min_page['content'])).sentiment\n",
    "\n",
    "print(\"Longest entry: \" + str(max_page_sa))\n",
    "print(\"Shortest entry: \" + str(min_page_sa))\n",
    "\n",
    "plt.title(\"Sentiment expressed in the pages\")\n",
    "plt.bar(['Max page Polarity','Max page Subjectivity','Min Page Polarity','Min Page Subjectivity'],[max_page_sa.polarity,max_page_sa.subjectivity,min_page_sa.polarity,min_page_sa.subjectivity])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What differences can you note in these two versions of the same page?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Run your own analysis\n",
    "\n",
    "We'll now look at a selection of pages from a different domain in that dataset. Here we will use [ontario.ca](https://ontario.ca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2_web_page_text = pd.read_csv(\"https://raw.githubusercontent.com/BrockDSL/ARCH_Data_Explore/main/snap_shot_ontario_ca.csv\")\n",
    "P2_web_page_text.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2_web_page_text['crawl_date']= pd.to_datetime(P2_web_page_text['crawl_date'],format='%Y%m%d')\n",
    "#add an extra column with how the length of each crawl. Useful for later calculations\n",
    "for index, row in P2_web_page_text.iterrows():\n",
    "    P2_web_page_text.at[index, \"length\"] = len(P2_web_page_text.at[index,\"content\"])\n",
    "    \n",
    "P2_web_page_text.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 25 URLs crawled in this Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2_web_page_text.groupby([\"url\"]).count().sort_values(by=\"crawl_date\",ascending=False)[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the [CSV file](https://raw.githubusercontent.com/BrockDSL/ARCH_Data_Explore/main/snap_shot_ontario_ca.csv) and look through it using Excel or something similar. Try to find and interesting URL that shows some changes in page length. You can experiment by setting the `P2_URL` variable in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2_URL = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2_url_data = P2_web_page_text[P2_web_page_text['url'] == P2_URL].sort_values(by=\"crawl_date\")\n",
    "\n",
    "#Plot out Word counts of crawls\n",
    "plt.plot(P2_url_data['crawl_date'],P2_url_data['length'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Word count variation by crawl for \\n\" + P2_URL)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Find longest and shortest page\n",
    "P2_max_page = P2_url_data[P2_url_data['length'] == P2_url_data['length'].max()]\n",
    "P2_max_page_date = str(P2_max_page['crawl_date'].values[0]).split('T')[0].replace('-','')\n",
    "\n",
    "\n",
    "print(\"\\n\\nLongest version of this page on the Internet Archive was captured \"\\\n",
    "      + P2_max_page_date + \"\\n\" \\\n",
    "      + \"Open this version on Internet Archive \\n\"\n",
    "      + \"https://web.archive.org/web/\" \\\n",
    "      + P2_max_page_date + \"/\" + P2_URL)\n",
    "\n",
    "\n",
    "P2_min_page = P2_url_data[P2_url_data['length'] == P2_url_data['length'].min()]\n",
    "P2_min_page_date = str(P2_min_page['crawl_date'].values[0]).split('T')[0].replace('-','')\n",
    "\n",
    "\n",
    "print(\"\\n\\nShortest version of this page on the Internet Archive was captured \"\\\n",
    "      + P2_min_page_date + \"\\n\" \\\n",
    "      + \"Open this version on Internet Archive \\n\"\n",
    "      + \"https://web.archive.org/web/\" \\\n",
    "      + P2_min_page_date + \"/\" + P2_URL)\n",
    "\n",
    "\n",
    "#Graph Sentiment\n",
    "P2_max_page_sa = TextBlob(str(P2_max_page['content'])).sentiment\n",
    "P2_min_page_sa = TextBlob(str(P2_min_page['content'])).sentiment\n",
    "\n",
    "print(\"Longest entry: \" + str(P2_max_page_sa))\n",
    "print(\"Shortest entry: \" + str(P2_min_page_sa))\n",
    "\n",
    "plt.title(\"Sentiment expressed in the pages\")\n",
    "plt.bar(['Max page Polarity','Max page Subjectivity','Min Page Polarity','Min Page Subjectivity'],[P2_max_page_sa.polarity,P2_max_page_sa.subjectivity,P2_min_page_sa.polarity,P2_min_page_sa.subjectivity])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the changes you see in the page between the shortest and longest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
