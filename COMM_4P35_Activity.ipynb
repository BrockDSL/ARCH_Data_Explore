{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMM 4P35 - Web Archives Tutorial\n",
    "\n",
    "\n",
    "## Part 1 - Analyzing changes to Canada.ca pages\n",
    "\n",
    "This notebook uses a subset of the the data from the [COVID in Niagara Archive](https://archive-it.org/collections/13781). We'll use Google Collab to explore how some pages from the [canada.ca](https://canada.ca) domain have changed during the course of the pandemic.\n",
    "\n",
    "Notebooks are comprised of 'cells'. Some are HTML other are code. If you click in a cell that is code you'll notice a 'play' button shows up in the left hand margin. Clicking on that play button will cause the code to run. \n",
    "\n",
    "Scroll through this page, reading the details and clicking on the play button in each one of the code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the the pieces\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "\n",
    "import difflib\n",
    "from IPython import display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.\n",
    "\n",
    "We'll load up the CSV file of data that represents our crawls of the canada.ca pages. We'll add some extra processing:\n",
    "\n",
    "- We calculate the length of each entry, and add as a new column\n",
    "- We calculate the sentiment of each entry, and as two new columns\n",
    "\n",
    "and randomly display one row of this spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Open up the CSV file fo data\n",
    "web_page_text = pd.read_csv(\"https://raw.githubusercontent.com/BrockDSL/ARCH_Data_Explore/main/snap_shot_canada_ca.csv\")\n",
    "\n",
    "\n",
    "#Make sure the date column is treated as a Date\n",
    "web_page_text['crawl_date']= pd.to_datetime(web_page_text['crawl_date'],format='%Y%m%d')\n",
    "\n",
    "\n",
    "#add an extra column with how the length of each crawl. Useful for later calculations\n",
    "for index, row in web_page_text.iterrows():\n",
    "    web_page_text.at[index, \"length\"] = len(web_page_text.at[index,\"content\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "##add two extra columns to the date that shows the calculated 'sentiment' of the entries\n",
    "\n",
    "polarity = []\n",
    "subjectivity = []\n",
    "\n",
    "\n",
    "for entry in web_page_text.content:\n",
    "    #print(day,\"\\n\")\n",
    "    score = TextBlob(entry)\n",
    "    polarity.append(score.sentiment.polarity)\n",
    "    subjectivity.append(score.sentiment.subjectivity)\n",
    "    \n",
    "web_page_text['polarity'] = polarity\n",
    "web_page_text['subjectivity'] = subjectivity\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# A random 'sample' of 1 record\n",
    "web_page_text.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of web pages captures in this archive subset: \" + str(len(web_page_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 \n",
    "\n",
    "Let's look at how many times the top 25 URLs in this archive have been crawled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_page_text.groupby([\"url\"]).count().sort_values(by=\"crawl_date\",ascending=False)[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Let's look at a specific URL... We set it in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=\"https://www.canada.ca/en/public-health/services/diseases/2019-novel-coronavirus-infection.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... with that set, let's plot out the change in content length of that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data = web_page_text[web_page_text['url'] == URL].sort_values(by=\"crawl_date\")\n",
    "\n",
    "plt.plot(url_data['crawl_date'],url_data['length'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Word count variation by crawl for \\n\" + URL)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "\n",
    "Curious. We see changes in the page length. Runing the cell below will generate a link to the Internet Archive for each different length version of this page and tell you when it was first harvested. Try a few of the links to see if you can spot what was added to the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unique_days = url_data.groupby(\"length\").first().sort_values(by='crawl_date')\n",
    "print(\"\\n\")\n",
    "for index, row in unique_days.iterrows():\n",
    "    date = str(row['crawl_date']).split(' ')[0].replace('-','')\n",
    "    length = len(row['content'])\n",
    "    print(\"Date of crawl: \",date, \". Length of page: \",length)\n",
    "    print(\"View on Internet archive https://web.archive.org/web/\" + date + \"/\" + URL)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Graphing Changes in Sentiment\n",
    "\n",
    "Let's map out the changes in sentiment scores for all of the capture dates for this URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(url_data['crawl_date'],url_data['polarity'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Polarity change for URL:\\n\"+URL)\n",
    "plt.ylabel(\"Polarity\")\n",
    "plt.xlabel(\"Date of Crawl\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(url_data['crawl_date'],url_data['subjectivity'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Subjectivity change for URL:\\n\"+URL)\n",
    "plt.ylabel(\"Subjectivity\")\n",
    "plt.xlabel(\"Date of Crawl\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To conclude\n",
    "\n",
    "By looking at various harvests of a particular page on the Internet Archive we can measure the change in sentiment that accompanies a change in page length. in this way we can see how additions and deletions to a page change both the content and the intention of the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Run your own analysis\n",
    "\n",
    "We'll now look at a selection of pages from a different domain in that dataset. Here we will use [ontario.ca](https://ontario.ca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2_web_page_text = pd.read_csv(\"https://raw.githubusercontent.com/BrockDSL/ARCH_Data_Explore/main/snap_shot_ontario_ca.csv\")\n",
    "P2_web_page_text.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2_web_page_text['crawl_date']= pd.to_datetime(P2_web_page_text['crawl_date'],format='%Y%m%d')\n",
    "#add an extra column with how the length of each crawl. Useful for later calculations\n",
    "for index, row in P2_web_page_text.iterrows():\n",
    "    P2_web_page_text.at[index, \"length\"] = len(P2_web_page_text.at[index,\"content\"])\n",
    "    \n",
    "P2_web_page_text.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 25 URLs crawled in this Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2_web_page_text.groupby([\"url\"]).count().sort_values(by=\"crawl_date\",ascending=False)[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to find and interesting URL in the list you just created that shows some changes in page length. You can experiment by setting the `P2_URL` variable in the next cell to that URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2_URL = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now run the next cell to perform the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2_url_data = P2_web_page_text[P2_web_page_text['url'] == P2_URL].sort_values(by=\"crawl_date\")\n",
    "\n",
    "#add an extra column with how the length of each crawl. Useful for later calculations\n",
    "for index, row in P2_url_data.iterrows():\n",
    "    P2_url_data.at[index, \"length\"] = len(P2_url_data.at[index,\"content\"])\n",
    "\n",
    "#Add two extra columns for sentiment scores\n",
    "P2_polarity = []\n",
    "P2_subjectivity = []\n",
    "\n",
    "for entry in P2_url_data.content:\n",
    "    #print(day,\"\\n\")\n",
    "    score = TextBlob(entry)\n",
    "    P2_polarity.append(score.sentiment.polarity)\n",
    "    P2_subjectivity.append(score.sentiment.subjectivity)\n",
    "    \n",
    "P2_url_data['polarity'] = P2_polarity\n",
    "P2_url_data['subjectivity'] = P2_subjectivity\n",
    "\n",
    "\n",
    "print(\"Analysis for: \",P2_URL,\"\\n\")\n",
    "\n",
    "#Find all changes in page length for this URL\n",
    "\n",
    "P2_unique_days = P2_url_data.groupby(\"length\").first().sort_values(by='crawl_date')\n",
    "\n",
    "for index, row in P2_unique_days.iterrows():\n",
    "    date = str(row['crawl_date']).split(' ')[0].replace('-','')\n",
    "    print(\"Date of crawl: \",date)\n",
    "    print(\"Length of page: \",len(row['content']))\n",
    "    print(\"Polarity: \", row['polarity'])\n",
    "    print(\"Subjectivity\",row['subjectivity'])\n",
    "    print(\"View on Internet archive https://web.archive.org/web/\" + date + \"/\" + URL)\n",
    "    print(\"\\n\")\n",
    "\n",
    "#Graph Sentiment\n",
    "\n",
    "#Plot out Word counts of crawls\n",
    "plt.plot(P2_url_data['crawl_date'],P2_url_data['length'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Word count variation by crawl for \\n\" + P2_URL)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(P2_url_data['crawl_date'],P2_url_data['polarity'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Polarity change for URL:\\n\"+P2_URL)\n",
    "plt.ylabel(\"Polarity\")\n",
    "plt.xlabel(\"Date of Crawl\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(P2_url_data['crawl_date'],P2_url_data['subjectivity'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Subjectivity change for URL:\\n\"+P2_URL)\n",
    "plt.ylabel(\"Subjectivity\")\n",
    "plt.xlabel(\"Date of Crawl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the changes you see in the page between the shortest and longest version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
