{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrockDSL/ARCH_Data_Explore/blob/main/Prep_Domain_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkzC_iG9Eqpt"
      },
      "source": [
        "# Preprocessing of DOMAIN data\n",
        "\n",
        "we'll grab all the content that matches list of `domains`. Calculate length, and VADER sentiment "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwKsPs_pGpaU",
        "outputId": "1e4f555e-9620-4fef-ece9-7e6651513f42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "Ready to proceed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import gdown\n",
        "\n",
        "#Sentiment just for fun, going to use VADER\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', False)\n",
        "pd.set_option('display.max_rows', 200)\n",
        "\n",
        "print(\"Ready to proceed\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the following values to restrict domains and set filename of CSV\n",
        "\n",
        "domains = [\n",
        "  \"twitter.com\"   \n",
        "\n",
        "]\n",
        "\n",
        "CSV_file = \"covid_twitter_data.csv\""
      ],
      "metadata": {
        "id": "sjb7N-AE2QXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhW-_jhOOc2q"
      },
      "outputs": [],
      "source": [
        "#Need to update to Dec 31 Crawl infoy\n",
        "gdown.download('https://drive.google.com/u/0/uc?id=1SzIpVc2fYLQTIasZZi3FnR7WhRRhsbJz&export=download','web-pages_full.csv.gz',quiet=False)\n",
        "!gunzip -k web-pages_full.csv.gz\n",
        "archive_text = pd.read_csv(\"web-pages_full.csv\")\n",
        "\n",
        "print(\"Data downloaded and loaded\")\n",
        "\n",
        "\n",
        "archive_text['crawl_date']= pd.to_datetime(archive_text['crawl_date'],format='%Y%m%d')\n",
        "archive_text = archive_text[archive_text['language'] =='en']\n",
        "\n",
        "#Selected Domains in sep datafram\n",
        "\n",
        "domains_text = archive_text[archive_text.domain.isin(domains)]\n",
        "domains_text = domains_text.reset_index()\n",
        "\n",
        "print(\"Number of records: \",len(domain_text))\n",
        "\n",
        "\n",
        "#length via itertuples, no index\n",
        "\n",
        "lengths = []\n",
        "for row in domain_text.itertuples(index=False):\n",
        "  lengths.append(int(len(row.content)))\n",
        "\n",
        "len_df = pd.DataFrame(lengths,columns=[\"length\"])\n",
        "\n",
        "domain_text = domain_text.join(len_df)\n",
        "print(\"Length calculations completed\")\n",
        "\n",
        "\n",
        "#VADER\n",
        "v_pos = []\n",
        "v_neg = []\n",
        "v_neu = []\n",
        "v_comp = []\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "for row in domain_text.itertuples(index=False):\n",
        "    ss = sid.polarity_scores(row.content)\n",
        "    v_pos.append(float(ss[\"pos\"]))\n",
        "    v_neg.append(float(ss[\"neg\"]))\n",
        "    v_neu.append(float(ss[\"neu\"]))\n",
        "    v_comp.append(float(ss[\"compound\"]))\n",
        "\n",
        "v_pos_df = pd.DataFrame(v_pos,columns=[\"v_pos\"])\n",
        "v_neg_df = pd.DataFrame(v_neg,columns=[\"v_neg\"])\n",
        "v_neu_df = pd.DataFrame(v_neu,columns=[\"v_neu\"])\n",
        "v_comp_df = pd.DataFrame(v_comp,columns=[\"v_comp\"])\n",
        "\n",
        "domain_text = muni_text.join(v_pos_df)\n",
        "domain_text = muni_text.join(v_neg_df)\n",
        "domain_text = muni_text.join(v_neu_df)\n",
        "domain_text = muni_text.join(v_comp_df)\n",
        "\n",
        "print(\"VADER scoring completed.\")\n",
        "\n",
        "domain_text.to_csv(CSV_file,index=False)\n",
        "files.download(CSV_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0EdFosNEdNQ"
      },
      "outputs": [],
      "source": [
        "#just to make sure, if necessary\n",
        "domain_text.sample(2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "Prep Domain Data",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1CJtlL0EgChUFSURaI5u-06kFcVhTyIXC",
      "authorship_tag": "ABX9TyNkmkJB17dr7lbX0itrDsjQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}